# Real-Time Sentiment Analysis via Knowledge Distillation

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)](https://huggingface.co/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

**Optimizing Large Language Models (LLMs) for real-time inference using Knowledge Distillation and Quantization.**

> **Key Achievement:** Compressed a RoBERTa-Large model into a MiniLM student, achieving **10Ã— faster inference** while retaining **>91% of the original accuracy**, enabling real-time deployment on standard hardware.

---
# ðŸ“– Overview

Large Language Models like RoBERTa deliver great accuracy but are too slow and heavy for **real-time applications** such as:
- live content moderation,
- on-device inference,
- real-time chatbot filtering.

This project implements a complete **Model Compression Pipeline**:

1. **Teacher Fine-Tuning** â€“ optimizing RoBERTa-Large on IMDb/TweetEval.  
2. **Knowledge Distillation** â€“ transferring the teacherâ€™s knowledge to compact models (MiniLM, DistilBERTâ€¦).  
3. **Hyperparameter Optimization** â€“ searching for the best temperature and Î± with Optuna.  
4. **Quantization** â€“ converting models to ONNX and applying INT8 dynamic quantization for speed.

### ðŸ§ª Datasets Used
- **IMDb** â€“ binary sentiment classification (Positive/Negative)  
- **TweetEval** â€“ 3-way sentiment (Positive / Negative / Neutral)

---

# ðŸ§  Distilled Models â€“ IMDB Sentiment Classification

Below are the distilled models trained for **binary sentiment analysis** on the **IMDb dataset**.  
Each model was distilled from a larger high-performance teacher (RoBERTa-Large).

| Model | Parameters | Test Accuracy | Hugging Face Repository |
|--------|-------------|----------------|--------------------------|
| **DistilRoBERTa (IMDB)** | ~82M | **92.80%** | ðŸ”— https://huggingface.co/Idrisdesu/distilled_distilroberta_imdb |
| **DistilBERT (IMDB)** | ~66M | **91.64%** | ðŸ”— https://huggingface.co/youssefennouri/distilled_distilbert_imdb |
| **MiniLM (IMDB)** | ~33M | **91.98%** | ðŸ”— https://huggingface.co/youssefennouri/distilled_minilm_imdb |
| **TinyBERT (IMDB)** | ~14M | **88.24%** | ðŸ”— https://huggingface.co/youssefennouri/distilled_tinybert_imdb |

### ðŸš€ Usage Example

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "youssefennouri/distilled_minilm_imdb"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

text = "The movie was surprisingly good and emotional."
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

prediction = outputs.logits.argmax().item()
print("Positive" if prediction == 1 else "Negative")
```

---

# ðŸ“Š Key Results & Insights

## 1. Performance vs. Speed Trade-off

| Model | Accuracy (IMDb) | Speedup | Size |
|-------|:--------------:|:-------:|:----:|
| **RoBERTa-Large (Teacher)** | **95.88%** | 1Ã— | ~1.4GB |
| DistilBERT | 92.5% | ~2Ã— | ~260MB |
| **MiniLM (Best Trade-off)** | **91.2%** | **~10Ã—** | **~120MB** |
| TinyBERT | 88.4% | ~20Ã— | ~60MB |

*(See `results/benchmarks/` for raw logs.)*

## 2. The â€œCalibrationâ€ Discovery

We discovered that **Teacher Accuracy â‰  Teaching Quality**.

- **IMDb** â†’ Teacher gave *overconfident* outputs â†’ students inherit the overfitting.  
- **TweetEval** â†’ Teacher produced *nuanced probabilities* â†’ better generalization for students like DistilBERT.

> **Takeaway:** A well-calibrated teacher produces much stronger students than a high-accuracy but overconfident teacher.

---

# ðŸ“‚ Project Structure

```text
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ training/       # Teacher fine-tuning & Knowledge Distillation
â”‚   â”‚   â”œâ”€â”€ fine_tuning_teacher_imdb.py
â”‚   â”‚   â”œâ”€â”€ fine_tuning_teacher_tweeteval.py
â”‚   â”‚   â”œâ”€â”€ distillation.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ inference/      # ONNX conversion, quantization, benchmarking
â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â”œâ”€â”€ inference_onnx.py
â”‚   â”‚   â”œâ”€â”€ quantize_model.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ teacher_confidence.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ distilbert_stats/
â”‚   â”œâ”€â”€ minilm_stats/
â”‚   â””â”€â”€ ...
â””â”€â”€ requirements.txt
```

---

ðŸš€ Installation

git clone https://github.com/votre-username/realtime-sentiment-distillation.git
cd realtime-sentiment-distillation
pip install -r requirements.txt

ðŸ“¥ Download Pre-trained Models

To reproduce our benchmarks immediately without training from scratch, you need to download the distilled and quantized models.

Option A: Automated Download (Recommended)
We provide a script to fetch all necessary models from Hugging Face and place them in the correct results/ structure.

python -m src.utils.download_models

(Note: If this script doesn't exist, please refer to Option B)

Option B: Manual DownloadIf you want to run the benchmarks, ensure your results/ folder looks like this. You can download the weights from the links in the "Distilled Models" section above or train them yourself using Step 2 and Step 3 in Usage.

Required structure for Benchmarking:

results/
â”œâ”€â”€ distilled_model_imdb/
â”‚   â”œâ”€â”€ distilled_distilbert_imdb/
â”‚   â”œâ”€â”€ distilled_minilm_imdb/
â”‚   â””â”€â”€ ...
â””â”€â”€ distilled_models_imdb_int8/  <-- (Generated via src.inference.quantize_model)
    â”œâ”€â”€ distilled_distilbert_imdb_int8_ptq_onnx/
    â””â”€â”€ ...

âš ï¸ Important: The ONNX quantized models (_int8_ptq_onnx) are hardware-specific. We strongly recommend generating them on your own machine:
---

# ðŸ›  Usage (How to Run)

âš ï¸ **IMPORTANT:** Always run using `python -m` **from the project root**, otherwise you will get `ModuleNotFoundError`.

---

## 1. Train the Teacher

```bash
# IMDb:
python -m src.training.fine_tuning_teacher_imdb

# Or TweetEval:
# python -m src.training.fine_tuning_teacher_tweeteval
```

## 2. Knowledge Distillation (Train Students)

```bash
python -m src.training.distillation --model distilbert --dataset imdb
```

(See `distillation.py` for model choices.)

## 3. Quantization & Benchmarking

```bash
# Standard inference benchmark
python -m src.inference.inference

# Quantize to ONNX INT8
python -m src.inference.quantize_model

# Benchmark ONNX model
python -m src.inference.inference_onnx
```

---

# ðŸ“ˆ Methodology Details

### 1. Teacher Fine-Tuning  
The teacher sets the performance **upper bound**.

### 2. Hyperparameter Search (Optuna)
We optimized:
- **Î±** â€“ balance between hard and soft labels  
- **temperature** â€“ softmax smoothing  

Logs:  
`results/distilbert_stats/hypersearch_*.csv`

### 3. Teacher Calibration Analysis  
Using `teacher_confidence.py`, we evaluated:
- maximum softmax probability (MSP),
- entropy of predictions,
- calibration curves.

This explained why some students learned surprisingly better on TweetEval.

---

# ðŸ‘¥ Authors

- **Idris NECHNECH**  
- **Youssef ENNOURI**  
- **Younes OUDINA**  

*Project completed as part of the NLP Course (2025).*
